
import json
import torch
import pandas as pd
from pathlib import Path
from datetime import datetime
import os
import sys
import numpy as np
import time

try:
    from src import config
    from src.cached_data_loader import CachedDataLoader
    from src.zone_manager import ZoneManager
    from src.tensor_builder import TensorBuilder
    from src.model.deepcon_stat import DeepConSTAT
except ImportError:
    import config
    from cached_data_loader import CachedDataLoader
    from zone_manager import ZoneManager
    from tensor_builder import TensorBuilder
    from model.deepcon_stat import DeepConSTAT

class ForecastEngine:
    def __init__(self, sort_by: str = 'spot_no'):
        # sort_by: 'spot_no' | 'name' | 'risk' passed to ZoneManager
        self.sort_by = sort_by
        self.zm = ZoneManager(sort_by=sort_by)
        self.tb = TensorBuilder(self.zm)
        
        # Initialize Model
        self.num_zones = self.zm.get_num_zones()
        self.model = DeepConSTAT(num_zones=self.num_zones)
        
        # Load pre-trained weights
        weights_path = Path("src/model/weights/best_model.pth")
        if weights_path.exists():
            try:
                self.model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))
                print(f"âœ… Loaded trained model weights from {weights_path}")
            except Exception as e:
                print(f"âš ï¸ Failed to load weights: {e}")
        else:
            print("âš ï¸ No trained weights found. Using random initialization.")
            
        self.model.eval() # Inference mode

    def predict_step(self, df: pd.DataFrame, t_point: int):
        """
        Runs a single-step inference for the given time point.
        Used by the Simulator for real-time-like playback.
        """
        # Context window: 60 minutes [t_point-60, t_point)
        start_idx = max(0, t_point - 60)
        df_chunk = df[(df['time_index'] >= start_idx) & (df['time_index'] < t_point)]

        if df_chunk.empty:
            return None, None, None

        # Build Tensor using absolute time indexing so features align to absolute bins
        tensor_chunk = self.tb.build_tensor(df_chunk, T=12, relative_time=True) # (12, Z, D) for 5-min x 1h (relative window)
        features = tensor_chunk.cpu().numpy()[-1, :, :] # (Z, D)
        
        with torch.no_grad():
            chunk_in = tensor_chunk.permute(1, 0, 2).unsqueeze(0) # (1, Z, 60, D)
            scores = self.model(chunk_in).squeeze() # (Z,)
            raw_scores = scores.cpu().numpy()
            
        # Apply Scaling (same as run_cycle for consistency)
        prior_multipliers = self.zm.get_prior_risk_vector()
        scaled_scores = np.zeros_like(raw_scores)
        
        for z in range(self.num_zones):
            p_weight = prior_multipliers[z]
            multiplier = 1.0 + (p_weight * 1.0) 
            raw_val = raw_scores[z]
            if raw_val > 0.001:
                scaled = (raw_val ** 0.5) * multiplier
            else:
                scaled = 0.0
            
            if scaled < 0.05:
                scaled = 0.0
            scaled_scores[z] = scaled
            
        # Global calibration isn't easily possible for a single step without daily context,
        # but we can apply a reasonable default scale or use the daily max if known.
        # For now, let's keep it normalized relative to a "Significant Activity" baseline of 0.7.
        # In a real simulator, we'd probably want to keep the same scale as the 24h forecast.
        
        return scaled_scores, features, df_chunk

    def run_cycle(self, target_date: str = None):
        # ìš”ì¼ íŒë³„ (ì£¼ì¤‘/ì£¼ë§)
        import datetime
        if target_date:
            try:
                dt = datetime.datetime.strptime(target_date, '%Y%m%d')
                weekday = dt.weekday() # 0=ì›”, ..., 5=í† , 6=ì¼
                is_weekend = weekday >= 5
            except Exception:
                is_weekend = False
        else:
            is_weekend = False
        print(f"[Forecast] {target_date} is_weekend={is_weekend}")
        """
        Executes the full forecasting cycle:
        1. Load latest cache data
        2. Build Tensor
        3. Run Inference
        4. Save Forecast
        """
        import time
        start_time = time.time()
        print(f"ğŸš€ Starting Forecast Cycle" + (f" for {target_date}" if target_date else ""))
        output_dir = Path("Cache")
        output_dir.mkdir(exist_ok=True)
        date_for_file = target_date if target_date else None

        # 1. Load Data
        t0 = time.time()
        loader = CachedDataLoader(config.CACHE_DIR, date_str=target_date)
        if not loader.is_valid():
            print("âŒ No valid cache found.")
            return

        # Load 5-min flow data - SAME AS T31/T41 TABS (Optimized with column pruning)
        try:
            # We only need these columns for TensorBuilder
            # CRITICAL: mac_address is required for unique worker/equipment counting
            # USE 5-MIN RESOLUTION to match T31/T41 tabs (already aggregated, no duplicates)
            needed_cols = ['time_index', 'type', 'spot_nos', 'mac_address', 'position_confidence', 'status']
            df = loader.load_flow_cache(resolution='5min', columns=needed_cols)
        except Exception as e:
            print(f"âš ï¸ Failed to load 5-min cache: {e}")
            return
            
        if df.empty:
            print("âš ï¸ Data is empty.")
            return
        t1 = time.time()
        print(f"â±ï¸ Data Load: {t1-t0:.2f}s")
        # data loaded

        # Load dwell time (ì²´ë¥˜ì‹œê°„) per zone (T41 ì‘ì—…ì) & anomaly ê¸°ì¤€ ê³„ì‚°
        try:
            dwell_df = loader.load_t41_worker_dwell()
            # Normalize dwell_df to a DataFrame when loader returns dict/list
            if isinstance(dwell_df, dict) or isinstance(dwell_df, list):
                try:
                    dwell_df = pd.DataFrame(dwell_df)
                except Exception:
                    dwell_df = pd.DataFrame()
            if not isinstance(dwell_df, pd.DataFrame):
                dwell_df = pd.DataFrame()
            zone_dwell = np.zeros(self.num_zones)
            dwell_anomaly = np.zeros(self.num_zones)
            # ê¸°ì¤€ê°’: ì£¼ì¤‘/ì£¼ë§ë³„ zone í‰ê·  dwell_minutes (ìºì‹œì—ì„œ ê³¼ê±° ë°ì´í„° í™œìš©)
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ Cache/zone_dwell_stats_{weekday|weekend}.jsonì—ì„œ ë¶ˆëŸ¬ì˜¨ë‹¤ê³  ê°€ì •
            import json
            stats_path = Path("Cache") / f"zone_dwell_stats_{'weekend' if is_weekend else 'weekday'}.json"
            if stats_path.exists():
                with open(stats_path, "r", encoding="utf-8") as f:
                    dwell_stats = json.load(f) # {zone_name: avg_dwell_minutes}
            else:
                dwell_stats = {}
            if not dwell_df.empty:
                # macë³„ë¡œ zone ì •ë³´ê°€ ìˆë‹¤ë©´ groupby('zone') ì‚¬ìš©
                if 'zone' in dwell_df.columns:
                    zone_group = dwell_df.groupby('zone')['dwell_minutes'].mean()
                    for idx, zname in enumerate(self.zm.get_zone_names()):
                        cur_dwell = zone_group.get(zname, 0)
                        base_dwell = dwell_stats.get(zname, 0)
                        zone_dwell[idx] = cur_dwell
                        # anomaly: ê¸°ì¤€ê°’ ëŒ€ë¹„ 1.3ë°° ì´ìƒì´ë©´ ë¹„ì •ìƒ
                        dwell_anomaly[idx] = 1 if base_dwell > 0 and cur_dwell > base_dwell * 1.3 else 0
                elif 'spot_no' in dwell_df.columns:
                    spot_group = dwell_df.groupby('spot_no')['dwell_minutes'].mean()
                    for idx in range(self.num_zones):
                        spot_no = self.zm.get_spot_no(idx)
                        cur_dwell = spot_group.get(spot_no, 0)
                        zname = self.zm.get_zone_names()[idx]
                        base_dwell = dwell_stats.get(zname, 0)
                        zone_dwell[idx] = cur_dwell
                        dwell_anomaly[idx] = 1 if base_dwell > 0 and cur_dwell > base_dwell * 1.3 else 0
                else:
                    avg_dwell = dwell_df['dwell_minutes'].mean()
                    zone_dwell[:] = avg_dwell
                    dwell_anomaly[:] = 0
            else:
                zone_dwell[:] = 0
                dwell_anomaly[:] = 0
        except Exception as e:
            import traceback
            print(f"âš ï¸ Failed to load dwell time: {e}")
            # Diagnostic: inspect dwell loader return and column value types
            try:
                import types
                print("[DWELL DIAG] Inspecting dwell_df variable...")
                # If dwell_df exists in locals, report its type and sample
                if 'dwell_df' in locals():
                    dd = dwell_df
                    try:
                        print("[DWELL DIAG] dwell_df type:", type(dd))
                        if hasattr(dd, 'shape'):
                            print(f"[DWELL DIAG] shape: {getattr(dd, 'shape', None)}")
                        print("[DWELL DIAG] dtypes:")
                        try:
                            print(dd.dtypes)
                        except Exception:
                            print("[DWELL DIAG] cannot print dtypes")
                        # For each column, show whether any cell is dict or list and sample
                        for col in getattr(dd, 'columns', [])[:10]:
                            try:
                                col_vals = dd[col].head(20).tolist()
                                types_present = {type(x) for x in col_vals}
                                print(f"[DWELL DIAG] col={col}, sample_types={types_present}")
                                # show first offending cell if dict found
                                for x in col_vals:
                                    if isinstance(x, dict):
                                        print(f"[DWELL DIAG] first dict in col={col}: {x}")
                                        break
                            except Exception:
                                print(f"[DWELL DIAG] could not inspect col={col}")
                    except Exception:
                        print("[DWELL DIAG] failed to introspect dwell_df")
                else:
                    print("[DWELL DIAG] dwell_df not defined in locals")
            except Exception:
                pass
            traceback.print_exc()
            zone_dwell = np.zeros(self.num_zones)
            dwell_anomaly = np.zeros(self.num_zones)
        
        # 2. Build Tensors & Run Sliding Inference (5-min intervals for Simulator fidelity)
        # 1440 mins / 5 mins = 288 intervals
        interval = 5
        time_points = list(range(interval, 1441, interval))  # 5, 10, 15, ..., 1440 (minutes)
        
        num_zones = len(self.zm.get_zone_names())
        num_steps = len(time_points)
        
        # Matrix to store ALL risk scores (Zone x TimeSteps)
        risk_matrix = np.zeros((num_zones, num_steps))
        # Store features for EVERY step for Simulator reasoning (T, Z, D)
        all_step_features = np.zeros((num_steps, num_zones, 4)) 
        
        print(f"âš¡ Processing 24h Risk Evolution ({num_steps} intervals, 5-min res)...")
        
        t2 = time.time()
        for i, t_point in enumerate(time_points):
            # Context window: 12 intervals (60 minutes) ending at current time_index
            # Convert minute-based t_point to 5-min time_index (1~288)
            # t_point is in minutes (5, 10, 15...), time_index is 1-based 5-min intervals
            current_idx = t_point // 5  # e.g., t_point=60 -> idx=12, t_point=1440 -> idx=288
            start_idx = max(1, current_idx - 12 + 1)  # 12 intervals = 1 hour window
            df_chunk = df[(df['time_index'] >= start_idx) & (df['time_index'] <= current_idx)]
            
            # Build Tensor using RELATIVE time indexing (maps chunk to 0..11)
            # relative_time=True ensures sparse data maps correctly to tensor positions
            tensor_chunk = self.tb.build_tensor(df_chunk, T=12, relative_time=True) # (12, Z, D) for 12 x 5-min intervals
            
            # Capture features for this step (last 5-min interval of the window)
            features = tensor_chunk.cpu().numpy()[-1, :, :]
            all_step_features[i] = features
            
            with torch.no_grad():
                chunk_in = tensor_chunk.permute(1, 0, 2).unsqueeze(0) # (1, Z, 12, D) - batch_size=1, zones, time_steps=12, channels=4
                scores = self.model(chunk_in).squeeze() # (Z,)
                risk_matrix[:, i] = scores.cpu().numpy()
            # periodic progress update (no-op in sync mode)

        t3 = time.time()
        print(f"â±ï¸ Total 24h Inference: {t3-t2:.2f}s")
        
        # 3. Integrated Risk Scoring: combine model output with interpretable signals
        # Components:
        #  - S_density: normalized worker density (0..1)
        #  - S_mix: equipment-to-worker mix proxy (0..1)
        #  - S_dwell: normalized dwell time (0..1)
        #  - M_zone: multiplicative zone prior multiplier
        #  - B_anomaly: binary anomaly booster (from dwell anomaly)
        norm_matrix = np.zeros_like(risk_matrix)
        prior_multipliers = self.zm.get_prior_risk_vector() # (Z,)
        zone_areas = self.zm.get_zone_areas()
        # weights for interpretable components (tuned to reduce over-sensitivity)
        w_density, w_mix, w_dwell = 0.4, 0.2, 0.4
        print("[DIAG] zone, time, raw_score, density, scaled_score (ìµœëŒ€ 10ê°œ)")
        diag_count = 0
        # baseline dwell lookup (may be empty)
        # dwell_stats loaded earlier may be {} if missing
        for z in range(num_zones):
            p_weight = prior_multipliers[z]
            M_zone = 1.0 + (p_weight * 0.5)
            B_anomaly = 1.0 + (0.5 if dwell_anomaly[z] else 0.0)
            for t in range(num_steps):
                raw_score = float(risk_matrix[z, t])
                p_feat = all_step_features[t, z, :] if all_step_features is not None else np.zeros(4)
                # p_feat channels store log1p((count/area)*100)
                feat_worker = float(np.expm1(p_feat[0]))  # equals (count/area)*100
                feat_equip = float(np.expm1(p_feat[1]))   # equals (equip_count/area)*100

                area = float(zone_areas[z]) if z < len(zone_areas) else 100.0
                # Recover density in persons/m2: (count/area) = feat_worker / 100
                density_ppm2 = feat_worker / 100.0
                # Smooth saturation: use a bounded non-linear mapping to avoid sharp jumps
                # S_density = density / (density + k) with k ~ 1.0 person/m2 (comfortable)
                S_density = float(min(1.0, density_ppm2 / (density_ppm2 + 1.0)))

                # S_mix: equipment per worker proxy. If workers=0, use equip/10 as proxy
                # Compute approximate counts to form equipment-to-worker mix
                worker_count = density_ppm2 * area
                equip_count = (feat_equip / 100.0) * area
                if worker_count > 0:
                    mix = equip_count / (worker_count + 1e-6)
                    S_mix = float(min(1.0, mix / 0.5))
                else:
                    S_mix = float(min(1.0, equip_count / 10.0))

                # S_dwell: zone-level dwell normalized by baseline from stats (if available)
                zname = self.zm.get_zone_names()[z]
                baseline = dwell_stats.get(zname, None) if isinstance(dwell_stats, dict) else None
                if baseline and baseline > 0:
                    S_dwell = float(min(1.0, zone_dwell[z] / (baseline + 1e-6)))
                else:
                    S_dwell = float(min(1.0, zone_dwell[z] / 10.0))

                # Feature-based combined score
                feature_score = w_density * S_density + w_mix * S_mix + w_dwell * S_dwell
                feature_score = float(min(1.0, feature_score))

                # Model component (use as-is when confident)
                if raw_score > 0.02:
                    model_comp = raw_score ** (0.75 if is_weekend else 0.8)
                    final_score = 0.6 * model_comp + 0.4 * feature_score
                else:
                    final_score = 0.95 * feature_score

                # Apply zone multiplier and anomaly booster
                final_score = final_score * M_zone * B_anomaly
                # small threshold to remove numerical noise
                if final_score < 0.01:
                    final_score = 0.0
                norm_matrix[z, t] = float(final_score)
                if diag_count < 10:
                    print(f"zone={z}, t={t}, raw={raw_score:.4f}, density={S_density:.4f}, scaled={final_score:.4f}")
                    diag_count += 1

        # 4. Global Peak Calibration (Target 0.7)
        # Cap amplification to avoid blowing up tiny model outputs into identical
        # mid-range scores. Allow modest downscaling if necessary.
        global_max = norm_matrix.max()
        if global_max > 0:
            desired = 0.7
            scale_factor = desired / global_max
            # prevent extreme amplification or complete collapse
            scale_factor = max(0.5, min(scale_factor, 2.0))
        else:
            scale_factor = 1.0

        norm_matrix = norm_matrix * scale_factor
        norm_matrix = np.clip(norm_matrix, 0, 1.0)
        
        # Calculate global statistics for anomaly detection
        global_mean = norm_matrix.mean()
        global_std = norm_matrix.std()
        
        # 5. Advanced Global AI Analysis
        peak_idx = np.unravel_index(np.argmax(norm_matrix, axis=None), norm_matrix.shape)
        peak_zone_idx, peak_time_idx = peak_idx
        peak_score = norm_matrix[peak_zone_idx, peak_time_idx]
        peak_zone_name = self.zm.get_zone_names()[peak_zone_idx]
        
        peak_hh = time_points[peak_time_idx] // 60
        peak_mm = time_points[peak_time_idx] % 60
        peak_time_str = f"{peak_hh:02d}:{peak_mm:02d}"
        
        # Global Insight generation (Peak reasoning)
        reason_msg = ""
        p_feat = all_step_features[peak_time_idx, peak_zone_idx]
        worker_den = np.expm1(p_feat[0])
        equip_den = np.expm1(p_feat[1])
        static_risk = p_feat[2]
        
        if worker_den > 30:
            reason_msg = f"í•´ë‹¹ êµ¬ì—­ì˜ **ì¸ì› ë°€ì§‘ë„(Worker Density: {worker_den:.1f})ê°€ ë§¤ìš° ë†’ê²Œ** ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. "
        elif equip_den > 5:
            reason_msg = f"êµ¬ì—­ ë‚´ **ì¤‘ì¥ë¹„ ê°€ë™(Equipment Activity: {equip_den:.1f})**ìœ¼ë¡œ ì¸í•œ ì¶©ëŒ ìœ„í—˜ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. "
        elif static_risk > 0.5:
            reason_msg = f"í•´ë‹¹ êµ¬ì—­ì˜ **ê³ ìœ  ìœ„í—˜ì„±(Static Risk: {static_risk:.1f}, ë°€í/ê³ ì†Œ)**ì´ ì£¼ì›ì¸ì…ë‹ˆë‹¤. "
        else:
            reason_msg = "ë³µí•©ì ì¸ í˜„ì¥ í™œë™ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. "

        # ì „ì²´ í‰ê·  ëŒ€ë¹„ í”¼í¬ ìœ„í—˜ë„ í‰ê°€
        peak_z_score = (peak_score - global_mean) / global_std if global_std > 0 else 0
        
        if peak_z_score > 2.5 and peak_score > 0.6:
            global_insight = f"ğŸš¨ **DeepCon AI ë¶„ì„ - CRITICAL**: ê¸ˆì¼ **{peak_time_str}**ì— **'{peak_zone_name}'** êµ¬ì—­ì—ì„œ **í‰ì†Œì™€ ë‹¤ë¥¸ ë¹„ì •ìƒì ì¸ ê³ ìœ„í—˜ ìƒí™©**ì´ ì˜ˆì¸¡ë©ë‹ˆë‹¤. {reason_msg}ê¸´ê¸‰ ì•ˆì „ ì ê²€ ë° ì‘ì—… ì¡°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif peak_score > 0.4:
            global_insight = f"âš ï¸ **DeepCon AI ë¶„ì„ - CAUTION**: ê¸ˆì¼ ê°€ì¥ ì£¼ì˜ê°€ í•„ìš”í•œ ì‹œì ì€ **{peak_time_str}**ì´ë©°, **'{peak_zone_name}'** êµ¬ì—­ì—ì„œ í”¼í¬ í™œë™ì´ ì˜ˆìƒë©ë‹ˆë‹¤. {reason_msg}í˜„ì¥ ì•ˆì „ ê´€ë¦¬ì— ìœ ì˜í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
        else:
            global_insight = f"âœ… **DeepCon AI ë¶„ì„ - SAFE**: ê¸ˆì¼ í˜„ì¥ì˜ ì „ë°˜ì ì¸ ë¦¬ìŠ¤í¬ëŠ” **ì•ˆì „í•œ(Safe)** ìˆ˜ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤. ì¼ìƒì ì¸ ì•ˆì „ ìˆ˜ì¹™ì„ ì¤€ìˆ˜í•´ ì£¼ì„¸ìš”."
 
        # 6. Process Results (Daily Max Snapshot)
        daily_max_scores = np.max(norm_matrix, axis=1) # (Z,)
        
        # í‰ì†Œ ëŒ€ë¹„ ë¹„ì •ìƒ ê°ì§€ë¥¼ ìœ„í•œ í†µê³„ ê³„ì‚°
        # í•˜ë£¨ ì „ì²´ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ íƒì§€
        daily_mean_scores = np.mean(norm_matrix, axis=1)  # ê° êµ¬ì—­ì˜ í•˜ë£¨ í‰ê· 
        daily_std_scores = np.std(norm_matrix, axis=1)    # ê° êµ¬ì—­ì˜ í‘œì¤€í¸ì°¨
        global_mean = np.mean(daily_max_scores)            # ì „ì²´ í‰ê· 
        global_std = np.std(daily_max_scores)              # ì „ì²´ í‘œì¤€í¸ì°¨
        
        results = []
        now_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        for idx, score in enumerate(daily_max_scores):
            spot_no = self.zm.get_spot_no(idx)
            peak_t_idx = np.argmax(norm_matrix[idx, :])
            p_hh = time_points[peak_t_idx] // 60
            p_mm = time_points[peak_t_idx] % 60
            peak_time_local = f"{p_hh:02d}:{p_mm:02d}"

            p_val = prior_multipliers[idx]
            dwell_val = zone_dwell[idx]
            anomaly_flag = dwell_anomaly[idx]
            
            # í•´ë‹¹ êµ¬ì—­ì˜ í‰ê·  ë° í‘œì¤€í¸ì°¨
            zone_mean = daily_mean_scores[idx]
            zone_std = daily_std_scores[idx] if daily_std_scores[idx] > 0 else 0.1
            
            # Z-score ê³„ì‚°: í‰ì†Œ ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚¬ëŠ”ì§€
            z_score = (score - zone_mean) / zone_std if zone_std > 0 else 0
            
            # ì „ì²´ í‰ê·  ëŒ€ë¹„ í¸ì°¨
            global_z = (score - global_mean) / global_std if global_std > 0 else 0
            
            # ìœ„í—˜ë„ ë ˆë²¨ íŒë‹¨ (ê°œì„ ëœ ë¡œì§)
            # 1. Critical: í‰ì†Œ ëŒ€ë¹„ 2 í‘œì¤€í¸ì°¨ ì´ìƒ ë²—ì–´ë‚˜ê±°ë‚˜, ì ˆëŒ€ê°’ì´ ë§¤ìš° ë†’ê³  ì´ìƒì¹˜ì¼ ë•Œ
            # 2. Caution: í•˜ë£¨ ì¤‘ ìµœê³ ì ì´ë©´ì„œ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¼ ë•Œ
            # 3. Safe: ë‚˜ë¨¸ì§€ ëŒ€ë¶€ë¶„ì˜ ê²½ìš°
            
            if (z_score > 2.0 or global_z > 2.5) and score > 0.6:
                # í‰ì†Œì™€ ë‹¤ë¥¸ ë¹„ì •ìƒì ì¸ ìƒí™©
                severity = "CRITICAL"
            elif score >= daily_max_scores.max() * 0.85 and score > 0.35:
                # í•˜ë£¨ ì¤‘ ìµœê³  ìˆ˜ì¤€ì˜ ìœ„í—˜ë„ (ìƒìœ„ 15% ì´ë‚´)
                severity = "CAUTION"
            elif anomaly_flag and z_score > 1.5:
                # ì²´ë¥˜ì‹œê°„ ì´ìƒ + ìœ„í—˜ë„ ì¦ê°€
                severity = "CAUTION"
            else:
                # ì¼ë°˜ì ì¸ ì•ˆì „í•œ ìƒí™©
                severity = "SAFE"

            # ìƒì„¸í•œ ì›ì¸ ë¶„ì„
            reasoning = f"[{severity}] ê¸ˆì¼ ìµœëŒ€ ìœ„í—˜ ë„ë‹¬ ì‹œê°„: {peak_time_local}. "
            
            # í”¼í¬ ì‹œì ì˜ ìƒì„¸ ë°ì´í„° ì¶”ì¶œ
            p_feat = all_step_features[peak_t_idx, idx]
            worker_density = float(np.expm1(p_feat[0]))  # ì¸ì› ë°€ì§‘ë„
            equip_density = float(np.expm1(p_feat[1]))   # ì¥ë¹„ ë°€ë„
            static_risk = float(p_feat[2])                # ê³ ìœ  ìœ„í—˜ë„
            
            area = float(zone_areas[idx]) if idx < len(zone_areas) else 100.0
            worker_count = (worker_density / 100.0) * area
            density_per_m2 = worker_density / 100.0
            
            # ì›ì¸ ìƒì„¸ ë¶„ì„
            reasons = []
            if severity == "CRITICAL":
                reasons.append(f"âš ï¸ **í‰ì†Œ ëŒ€ë¹„ {z_score:.1f}ë°° ë†’ì€ ìœ„í—˜ë„ ê°ì§€** (ì´ìƒì¹˜)")
            
            if worker_density > 50:
                reasons.append(f"**ê³¼ë°€ ìƒíƒœ**: ë©´ì  ëŒ€ë¹„ ì¸ì› ë°€ì§‘ë„ {density_per_m2:.2f}ëª…/mÂ² (ì•½ {worker_count:.0f}ëª…, ê¶Œì¥ ê¸°ì¤€ ì´ˆê³¼)")
            elif worker_density > 30:
                reasons.append(f"**ì¸ì› ì§‘ì¤‘**: ë©´ì  ëŒ€ë¹„ {density_per_m2:.2f}ëª…/mÂ² (ì•½ {worker_count:.0f}ëª…)")
            elif worker_density > 10:
                reasons.append(f"**ì¼ë°˜ ì‘ì—…**: ë©´ì  ëŒ€ë¹„ {density_per_m2:.2f}ëª…/mÂ² (ì•½ {worker_count:.0f}ëª…)")
            
            if equip_density > 8:
                reasons.append(f"**ì¤‘ì¥ë¹„ ë°€ì§‘**: ì¥ë¹„ ê°€ë™ë¥  ë†’ìŒ (ì¶©ëŒ/í˜‘ì°© ìœ„í—˜ ì¦ê°€)")
            elif equip_density > 3:
                reasons.append(f"**ì¥ë¹„ ìš´ìš© ì¤‘**: ì‘ì—…ì-ì¥ë¹„ ê°„ ì•ˆì „ê±°ë¦¬ ìœ ì§€ í•„ìš”")
            
            if static_risk > 0.7:
                reasons.append(f"**ê³ ìœ„í—˜ êµ¬ì—­**: ë°€íê³µê°„/ê³ ì†Œì‘ì—… ë“± ìƒì‹œ ìœ„í—˜ìš”ì†Œ ì¡´ì¬")
            elif static_risk > 0.4:
                reasons.append(f"**ì£¼ì˜ êµ¬ì—­**: êµ¬ì¡°ì  ìœ„í—˜ìš”ì†Œ ìˆìŒ")
            
            if anomaly_flag:
                reasons.append(f"**ë¹„ì •ìƒ ì²´ë¥˜**: í‰ì†Œ ëŒ€ë¹„ ì²´ë¥˜ì‹œê°„ ê¸¸ì–´ì§ (í‰ê·  {dwell_val:.0f}ë¶„, ì‘ì—… ì§€ì—° ë˜ëŠ” ë¬¸ì œ ë°œìƒ ê°€ëŠ¥ì„±)")
            
            if reasons:
                reasoning += " ".join(reasons)
            else:
                reasoning += "ì •ìƒì ì¸ ì‘ì—… íŒ¨í„´ ìœ ì§€ ì¤‘. ì¼ìƒì ì¸ ì•ˆì „ ìˆ˜ì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”."

            results.append({
                "spot_id": int(spot_no),
                "zone_name": self.zm.get_zone_names()[idx],
                "risk_score": float(score),
                "severity": severity,
                "reasoning": reasoning,
                "timestamp": now_str
            })
            
        time_labels = [f"{tp//60:02d}:{tp%60:02d}" for tp in time_points]
            
        output_data = {
            "forecasts": results,
            "heatmap": {
                "z": norm_matrix.tolist(),
                "y": self.zm.get_zone_names(),
                "x": time_labels
            },
            "step_features": all_step_features.tolist(), # (T, Z, D)
            "global_analysis": {
                "peak_time": peak_time_str,
                "peak_zone": peak_zone_name,
                "peak_score": float(peak_score),
                "insight": global_insight
            }
        }
        # Add explicit zone_name -> spot_id mapping for downstream consumers
        zone_names = self.zm.get_zone_names()
        name_to_spot = {}
        for idx, zname in enumerate(zone_names):
            try:
                sid = int(self.zm.get_spot_no(idx))
            except Exception:
                sid = None
            if sid is not None:
                name_to_spot[zname] = sid
        output_data['zone_mapping'] = name_to_spot
            
        # Save to JSON (use date tag)
        date_for_file = target_date if target_date else loader.date_str
        output_file = output_dir / f"forecast_{date_for_file}.json"
        
        with open(output_file, "w", encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # Also keep 'latest_forecast.json' as a pointer to the newest one
        latest_file = output_dir / "latest_forecast.json"
        with open(latest_file, "w", encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        # Also save a dedicated zone mapping file for quick lookup
        try:
            mapping_file = output_dir / f"zone_mapping_{date_for_file}.json"
            with open(mapping_file, 'w', encoding='utf-8') as mf:
                json.dump(output_data.get('zone_mapping', {}), mf, ensure_ascii=False, indent=2)
        except Exception:
            pass

        # completed

        total_time = time.time() - start_time
        print(f"âœ… Refined 24h Forecast saved to {output_file} (Total: {total_time:.2f}s)")

        # ê°œì„ ëœ ì´ìƒ ê°ì§€ ë©”ì‹œì§€ (í‰ì†Œ ëŒ€ë¹„ ë¹„ì •ìƒë§Œ ë³´ê³ )
        anomalies = []
        
        # ì „ì—­ í”¼í¬ê°€ í‰ì†Œ ëŒ€ë¹„ ì´ìƒì¹˜ì¸ ê²½ìš°ë§Œ
        peak_z_score = (peak_score - global_mean) / global_std if global_std > 0 else 0
        if peak_z_score > 2.5 and peak_score > 0.6:
            anomalies.append({
                'level': 'CRITICAL',
                'message': f'[ë¹„ì •ìƒ ê°ì§€] {peak_zone_name} êµ¬ì—­ {peak_time_str} ì‹œì ì—ì„œ í‰ì†Œ ëŒ€ë¹„ {peak_z_score:.1f}Ïƒ ë†’ì€ ìœ„í—˜ë„({peak_score:.2f}) ê°ì§€. ê¸´ê¸‰ ì ê²€ í•„ìš”.'
            })
        elif peak_score > 0.5:
            anomalies.append({
                'level': 'WARNING',
                'message': f'[ì£¼ì˜] {peak_zone_name} êµ¬ì—­ {peak_time_str} ì‹œì  í”¼í¬ í™œë™({peak_score:.2f}). ì•ˆì „ ê´€ë¦¬ ê°•í™” ê¶Œì¥.'
            })

        # êµ¬ì—­ë³„ CRITICALë§Œ ë³´ê³  (í‰ì†Œ ëŒ€ë¹„ ì´ìƒì¹˜)
        for r in results:
            if r.get('severity') == 'CRITICAL':
                anomalies.append({
                    'level': 'CRITICAL',
                    'message': f"[ì´ìƒì¹˜] {r['zone_name']} (spot {r['spot_id']}) í‰ì†Œ ëŒ€ë¹„ ë†’ì€ ìœ„í—˜ë„({r['risk_score']:.2f}) - ì¦‰ì‹œ í™•ì¸ í•„ìš”"
                })

        output_data['anomalies'] = anomalies

        return output_data

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--date", type=str, help="Target date (YYYYMMDD)")
    args = parser.parse_args()
    
    engine = ForecastEngine()
    engine.run_cycle(target_date=args.date)
